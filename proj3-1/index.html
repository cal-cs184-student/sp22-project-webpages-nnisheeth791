<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
	<style>
		body {
			padding: 100px;
			width: 1000px;
			margin: auto;
			text-align: left;
			font-weight: 300;
			font-family: 'Open Sans', sans-serif;
			color: #121212;
		}

		h1,
		h2,
		h3,
		h4 {
			font-family: 'Source Sans Pro', sans-serif;
		}
	</style>
	<title>CS 184 Mesh Editor</title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2022</h1>
<h1 align="middle">Project 3-1: PathTracer</h1>
<h2 align="middle">Naman Nisheeth (SID: 3034144312) and Owen Gong (SID: 3034100450)</h2>
<h3 align="middle"> <a href="https://cal-cs184-student.github.io/sp22-project-webpages-nnisheeth791/proj3-1/index.html">Link to Hosted Website:</a> https://cal-cs184-student.github.io/sp22-project-webpages-nnisheeth791/proj3-1/index.html</h3>

<br><br>

<div>

	<h2 align="middle">Overview</h2>
	<p>In this project, we implemented to main foundations of path tracing. We created the core rountines of a physically-based
	rendering system that used a pathtracing algorithm. There were many concepts that we learned in class which we applied in this
	assignment, such as ray-scene intersection, acceleration structures, and physically based lighting and materials. This was a technically
	challenging project simply due to the time it took to render some objects.</p>

	<h3 align="middle">Part 1: Ray Generation and Scene Intersection</h3>

	<p>The first thing we had to do in the project was generate camera rays. We would take normalized image coordinates ass inputs
	and output them as Rays in the world space. We did this by transforming the image coordinates to camera space, generating the
	rays to camera space, and finally transforming them to a Ray object in the world space. We used the Moller Trumbore Algorithm to
	implement our intersection and repeated them to be able to reset the intersect structure. With this, we could express our triangle
	in barycentric coordinates. If the t-value is between min_t and max_t, then we can conclude that an intersection has occurred. We then
	store this value in the intersect structure.</p>

	<div align="middle">
		<table style="width=100%">
			<tr>
				<td>
					<img src="images/banana.JPEG" align="middle" width="300px" />
					<figcaption align="middle">BANANAS</figcaption>
				</td>
				<td>
					<img src="images/CBempty.JPEG" align="middle" width="300px" />
					<figcaption align="middle">EMPTY BOX</figcaption>
				</td>
			</tr>
			<tr>
				<td>
					<img src="images/CBspheres.JPEG" align="middle" width="300px" />
					<figcaption align="middle">SPHERES</figcaption>
				</td>
			</tr>
		</table>
	</div>



	<h3 align="middle">Part 2: Bounding Volume Hierarchy</h3>
	<p>The BVH construct algorithm that we made focused on using the average centroid value as the splitting factor. We believed it was
	an effective heuristic and it produced good results for our images and rendered well. We used the partition function to determine
	when to start and stop between the left and right splits. We also used the distance function to determine if there were any areas
	that had no primitives and would thus cause infinite recursion. If centroids were greater than the heuristic, I would put them in
	the right tree, otherwise I would put them in the left tree.</p>

	<div align="middle">
		<table style="width=100%">
			<tr>
				<td>
					<img src="images/cow.JPEG" align="middle" width="300px" />
					<figcaption align="middle">Rendering Time: 0.0472s</figcaption>
				</td>
				<td>
					<img src="images/maxplanck.JPEG" align="middle" width="300px" />
					<figcaption align="middle">Rendering Time: 0.1753s</figcaption>
				</td>
			</tr>
			<tr>
				<td>
					<img src="images/CBlucy.JPEG" align="middle" width="300px" />
					<figcaption align="middle">Rendering Time: 0.0834s</figcaption>
				</td>
			</tr>
		</table>
	</div>

	<p>BVH was crucial for a palatable rendering speed. Without BVH, images like CBlucy and maxplanck wouldn't run and cow would take a very long
	time compared to now. BVH helped with splitting up nodes into a tree, sorting them into left and right. It also did a good job with discarding
		bounding boxes that were empty, which helped the performance. Based on the time differences
	when it came to rendering, we can say that we successfully sped up the rendering from O(n) to O(log(n)) time. There were many factors
	that contribute to the overall rendering time, including how we implemented many of the functions in part 1 such as raytrace_pixel. I think
	that modifying that function for part 2 and 3 really assisted with our speedup as well.</p>


	<h3 align="middle">Part 3: Direct Illumination</h3>
	<p>In this part, we simulated light transport and rendered images with realistic shading. We implemented two direct illumination methods:
	uniform hemisphere and importance sampling.</p>

	<p>For uniform hemisphere sampling, we implemented a methods that would estimate the direct lighting on a point by sampling
	uniformly in a hemisphere. First, we converted the image to world coordinates and then calculated the direction and origin. If there
	was an intersection, we then computed the amount of light entering with the Monte Carlo estimator which was described in the spec.
	This was the result of running it on the CBempty image:</p>

	<div align="middle">
		<table style="width=100%">
			<img src="images/emptyUHS.png" align="middle" width="400px" />
		</table>
	</div>

	<p>For importance sampling, we wanted to sample directions between the light sources and hit_p. This is overall produces a more clear
	image than uniform hemisphere sampling and has much less noise. The approach was very similar to uniform hemisphere sampling We also
	used the Monte Carlo estimator in this method, but we checked to see if the light was a delta light and if it was, we would sample once
	since all samples would be the same.</p>

	<div align="middle">
		<table style="width=100%">
			<img src="images/emptyIS.png" align="middle" width="400px" />
		</table>
	</div>

	<p>After rending the CBbunny image multiple times with different amounts of light rays, we came to the conclusion that more light rays
	made the image more clear and less noisy, similar to the effect of the importance sampling.</p>

	<div align="middle">
		<table style="width=100%">
			<tr>
				<td>
					<img src="images/bunny1.png" align="middle" width="300px" />
					<figcaption align="middle">1 light ray</figcaption>
				</td>
				<td>
					<img src="images/bunny4.png" align="middle" width="300px" />
					<figcaption align="middle">4 light rays</figcaption>
				</td>
			</tr>
			<tr>
				<td>
					<img src="images/bunny16.png" align="middle" width="300px" />
					<figcaption align="middle">16 light rays</figcaption>
				</td>
				<td>
					<img src="images/bunny64.png" align="middle" width="300px" />
					<figcaption align="middle">64 light rays</figcaption>
				</td>
			</tr>
		</table>
	</div>

	<p>Overall, importance sampling definitely provides a more noise-free image than uniform hemisphere sampling (UHS) does. However,
	UHS does a better job with displaying shadow, glows, and textures than importance sampling, at the cost of
	more noise. Since UHS samples only the hemisphere, a lot of spots are missed for light rays and this creates the dark, grainy bits
	we see a lot in the UHS images. Importance sampling is more time intensive, but it uses more information to provide a clearer image.</p>



	<h3 align="middle">Part 4: Global Illumination</h3>

	<p>Next, we implemented global illumination, which expands upon direct lighting by accounting for the possibility of light bouncing on multiple
		surfaces, rather than just one. To initialize the illumination, we call one_bounce_radiance() from task 3, and then recursively call the function
		depending on if the max ray depth is less than 1 or the new bounced ray doesn't intersect the scene. We also terminate recursion through a random termination method dubbed "Russian Roulette",
		in which regardless of ray depth we terminate the function by a random probability coin flip. We chose the probability 0.3 per spec recommendation.
		Finally, we update the resulting hit point depending on whether or not the ray has reached max ray depth by multiplying w_in z component by bsdf sample f by the resulting radiance, and dividing everything by the pdf.
		If the ray did not reach maximum depth, we also divide by (1 - termination probability) based on Russian Roulette.

	</p>

	<p>Below are some images rendered with global illumination (both direct and indirect) at 1024 samples per pixel: </p>


	<div align="middle">
		<table style="width=100%">
			<tr>
				<td>
					<img src="images/spheres.png" align="middle" width="300px" />
					<figcaption align="middle">CBspheres_lambertian.dae</figcaption>
				</td>
				<td>
					<img src="images/dragon.png" align="middle" width="300px" />
					<figcaption align="middle">dragon.dae</figcaption>
				</td>
				<td>
					<img src="images/blob.png" align="middle" width="300px" />
					<figcaption align="middle">blob.dae</figcaption>
				</td>
			</tr>
		</table>
	</div>

	Here's a scene comparing rendered views of CBspheres_lambertian.dae with ONLY Direct illumination and then ONLY indirect illumination at 1024 samples per pixel:
	<div align="middle">
		<table style="width=100%">
			<tr>
				<td>
					<img src="images/directReal.png" align="middle" width="300px" />
					<figcaption align="middle">Only Direct Lighting</figcaption>
				</td>
				<td>
					<img src="images/indirect.png" align="middle" width="300px" />
					<figcaption align="middle">Only Indirect Lighting</figcaption>
				</td>
			</tr>
		</table>
	</div>

	Here is CBbunny.dae rendered views with max_ray_depth set at various values at 1024 samples per pixel:

	<div align="middle">
		<table style="width=100%">
			<tr>
				<td>
					<img src="images/bunny0.png" align="middle" width="200px" />
					<figcaption align="middle">Depth: 0</figcaption>
				</td>
				<td>
					<img src="images/.png" align="middle" width="200px" />
					<figcaption align="middle">Depth: 1</figcaption>
				</td>
			</tr>
			<tr>
				<td>
					<img src="images/.png" align="middle" width="200px" />
					<figcaption align="middle">Depth: 2</figcaption>
				</td>
				<td>
					<img src="images/.png" align="middle" width="200px" />
					<figcaption align="middle">Depth: 3</figcaption>
				</td>
			</tr>
			<tr>
				<td>
					<img src="images/.png" align="middle" width="200px" />
					<figcaption align="middle">Depth: 100</figcaption>
				</td>
			</tr>
		</table>
	</div>

	Here is CBspheres_lambertian.dae with 4 light rays at various sample-per-pixel rates:
	<div align="middle">
		<table style="width=100%">
			<tr>
				<td>
					<img src="images/sphere1.png" align="middle" width="300px" />
					<figcaption align="middle">Sample per Pixel: 1</figcaption>
				</td>
				<td>
					<img src="images/sphere2.png" align="middle" width="300px" />
					<figcaption align="middle">Sample per Pixel: 2</figcaption>
				</td>
			</tr>
			<tr>
				<td>
					<img src="images/sphere4.png" align="middle" width="300px" />
					<figcaption align="middle">Sample per Pixel: 4</figcaption>
				</td>
				<td>
					<img src="images/sphere8.png" align="middle" width="300px" />
					<figcaption align="middle">Sample per Pixel: 8</figcaption>
				</td>
			</tr>
			<tr>
				<td>
					<img src="images/sphere16.png" align="middle" width="300px" />
					<figcaption align="middle">Sample per Pixel: 16</figcaption>
				</td>
				<td>
					<img src="images/sphere64.png" align="middle" width="300px" />
					<figcaption align="middle">Sample per Pixel: 64</figcaption>
				</td>
			</tr>
			<tr>
				<td>
					<img src="images/sphere1024.png" align="middle" width="300px" />
					<figcaption align="middle">Sample per Pixel: 1024</figcaption>
				</td>
			</tr>
		</table>
	</div>


	<h3 align="middle">Part 5: Adaptive Sampling</h3>
	<p>Finally, we implemented adaptive sampling by modifying logic in raytrace_pixel to account for convergence when ray tracing.
		Pixels that converge quickly do not need as many samples as those that don't converge, and we can adjust that accordingly in our code
		by checking if convergence is less than maxTolerence * (the mean of n samples) with equations given in spec (equations depend on mean and variance of the n samples,
		which in turn depends on the sum of the samples' illuminance and sum of the samples' illuminance squared. Because we don't want to calculate a pixel's convergence for
		each new sample, we also want to check in batches (such as 32, 64 samples in a batch at a time), to improve time performance. The resulting sample rate image shows us
		which pixels and areas are sampled the most (converge the slowest), with red indicating more sample and blue indicating less samples.
	</p>


	<p>Below is CBbunny.dae rendered at 2048 samples per pixel (64 samples per batch), at 1 sample per light and 5 max ray depth.
		Though the sample rate image seems a little off from the reference picture in the docs (reference has more green on the area light on top),
		the image still clearly shows areas of higher sampling rate (the red ceiling and bottom of bunny) vs. areas of lower sampling rate (the walls and area light).</p>
	<div align="middle">
		<table style="width=100%">
			<tr>
				<td>
					<img src="images/bunnyRate.png" align="middle" width="300px" />
					<figcaption align="middle">Sample Rate Image</figcaption>
				</td>
				<td>
					<img src="images/bunny.png" align="middle" width="300px" />
					<figcaption align="middle">Noise-free Rendered Result</figcaption>
				</td>
			</tr>
		</table>
	</div>

</body>

</html>